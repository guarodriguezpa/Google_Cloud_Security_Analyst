Analyze audit logs using BigQuery

### 

**Cloud Incident Investigation and Log Analysys with BiqQuery (Google
Cloud Lab)**

In this Google Cloud lab, I gained hands-on experience in **security
incident investigation** by analyzing unauthorized access to cloud
resources.

As a Cloud Security Analyst at Cymbal Bank, my objective was to
understand a high-severity alert related to compromised cloud resources
by recreating the incident in a test environment and analyzing the
generated audit logs.

My actions involved recreating a security incident to observe malicious
activity firsthand.

I began by **generating suspicious activity from a first user account**,
simulating unauthorized access to cloud resources. This allowed me to
create relevant log entries. Next, I **exported these logs for further
analysis**, preparing the data for investigation. I continued to
**recreate parts of the incident by generating additional user
activity** to produce a more complete set of logs.

Finally, I leveraged **Google BigQuery to analyze these logs**,
identifying patterns of suspicious activity and understanding the \"who,
what, where, and when\" of the incident. This analysis was crucial for
understanding the attack flow and supporting incident response efforts.

### **Activate Cloud Shell**

Cloud Shell is an online development and operations environment
accessible anywhere with your browser. Cloud Shell provides command-line
access to your Google Cloud resources.

1.  Click **Activate Cloud Shell** ![image](https://github.com/user-attachments/assets/6c7e4002-07d6-4306-972d-a73497ed115b)
 at the top right of the Google
    Cloud console. You may be asked to click **Continue**.

After Cloud Shell starts up, you\'ll see a message displaying your
Google Cloud Project ID for this session:

Your Cloud Platform project in this session is set to YOUR_PROJECT_ID

The command-line tool for Google Cloud, gcloud,comes pre-installed on
Cloud Shell and supports tab-completion. In order to access Google
Cloud, you\'ll have to first authorize gcloud.

2.  List the active account name with this command:

\###

gcloud auth list

\####

3.  A pop-up will appear asking you to **Authorize Cloud Shell**.
    Click **Authorize**.

4.  Your output should now look like this:

![image](https://github.com/user-attachments/assets/13481843-25ba-4322-bf17-0323740be6b1)


**Output:**

List the project ID with this command:

\###

gcloud config list project

\#####

![image](https://github.com/user-attachments/assets/a745dc99-857b-43f8-a6fe-0224e4508e61)


## Task 1. Generate account activity

In this task, I\'ll create and delete cloud resources to generate
account activity which you\'ll access as Cloud Audit Logs.

1.  Copy the following commands into the Cloud Shell terminal:

\#####

gcloud storage buckets create gs://\$DEVSHELL_PROJECT_ID

echo \"this is a sample file\" \> sample.txt

gcloud storage cp sample.txt gs://\$DEVSHELL_PROJECT_ID

gcloud compute networks create mynetwork \--subnet-mode=auto

export ZONE=\$(gcloud compute project-info describe \\

\--format=\"value(commonInstanceMetadata.items\[google-compute-default-zone\])\")

gcloud compute instances create default-us-vm \\

\--machine-type=e2-micro \\

\--zone=\$ZONE \--network=mynetwork

gcloud storage rm \--recursive gs://\$DEVSHELL_PROJECT_ID

\########

2.  Press **ENTER**.

![image](https://github.com/user-attachments/assets/ade82c77-2a90-4740-b0b4-7f749df473c8)

## Task 2. Export the audit logs

The activity generated in the previous task was recorded as audit logs.
In this task I\'ll export these logs to a BigQuery dataset for further
analysis.

1.  In the Google Cloud console, in the **Navigation
    menu** ![image](https://github.com/user-attachments/assets/7301a230-60fc-4773-9dd8-25dbc8bb455b)
 click **Logging \> Logs Explorer**.
    The **Logs Explorer** page opens

2.  When exporting logs, the current filter will be applied to what is
    exported. Copy the following query into the **Query** builder:

> \#####

logName =
(\"projects/qwiklabs-gcp-01-c4796fed48e3/logs/cloudaudit.googleapis.com%2Factivity\")

\######

3.  Click **Run query**. The query results should display on the **Query
    results** pane. This query filters for Cloud Audit logs within your
    project.

![image](https://github.com/user-attachments/assets/767802a8-cc22-45dd-b92f-cdb579e4edeb)


4.  Under the **Query editor** field, click **More actions \> Create
    sink.** The **Create logs routing sink** dialog opens.

![image](https://github.com/user-attachments/assets/a25713e6-6219-476f-8377-79fc41738512)


5.  In the **Create logs routing sink** dialog, specify the following
    settings and leave all other settings at their defaults:

  -------------------------------------------------------------------------
  **Section**          **Field: values**
  -------------------- ----------------------------------------------------
  **Sink details**     Sink name: AuditLogsExport\
                       Click **Next.**

  **Sink destination** **Select sink service:** BigQuery dataset\
                       **Select BigQuery dataset:** Create new BigQuery
                       dataset.\
                       The **Create dataset** dialog opens.

  **Create dataset**   **Dataset ID:** auditlogs_dataset\
                       Click **Create Dataset**.\
                       The **Create dataset** dialog closes, and you\'ll
                       return to the **Sink destination** dialog.

  **Sink destination** Click **Next**.\
                       Uncheck the **Use Partitioned Tables** checkbox, if
                       it is already selected, and click **Next**.

  **Choose logs to     Notice the pre-filled Build inclusion
  include in sink**    filter:logName=(\"projects/\[PROJECT
                       ID\]/logs/cloudaudit.googleapis.com%2Factivity\")\
                       Click **Next**.\
                       Click **Create Sink**.\
                       Return to the **Logs Explorer** page.
  -------------------------------------------------------------------------

![image](https://github.com/user-attachments/assets/eb396255-7218-46f8-afd3-24983b7c5634)

>
![image](https://github.com/user-attachments/assets/3fb92971-f89c-4c20-b2ef-d1751c400ae0)

>
![image](https://github.com/user-attachments/assets/2d786e01-66cd-4f90-acdd-42cf3a04cf31)

>
![image](https://github.com/user-attachments/assets/11108f74-585e-47fc-a3a1-ba9bbb176f32)

>
> In the **Logging** navigation pane, click **Log Router** to view
> the **AuditLogsExport** sink in the **Log Router Sinks** list.
>
![image](https://github.com/user-attachments/assets/4d2416cc-484c-466e-856d-abf0dbbb1121)


6.  Inline with the **AuditLogsExport** sink, click **More
    actions** ![image](https://github.com/user-attachments/assets/be5fcf1b-3087-4fab-a3d2-d9bfcb9be3a0)
) \> **View sink details** to view
    information about the **AuditLogsExport** sink you created.
    The **Sink details** dialog opens.

7.  Click **Cancel** to close the **Sink details** dialog when you\'re
    done viewing the sink information.

![image](https://github.com/user-attachments/assets/baf6123a-79c0-4ae0-9b4b-ba473688daba)


All future logs will now be exported to BigQuery, and the BigQuery tools
can be used to perform analysis on the audit log data. The export does
not export existing log entries.

## Task 3. Generate more account activity

In this task, I\'ll create and delete cloud resources to generate
additional account activity which you\'ll then access in BigQuery to
extract additional insights from the logs.

1.  Copy the following commands into the Cloud Shell terminal:

\####

gcloud storage buckets create gs://\$DEVSHELL_PROJECT_ID

gcloud storage buckets create gs://\$DEVSHELL_PROJECT_ID-test

echo \"this is another sample file\" \> sample2.txt

gcloud storage cp sample.txt gs://\$DEVSHELL_PROJECT_ID-test

export ZONE=\$(gcloud compute project-info describe \\

\--format=\"value(commonInstanceMetadata.items\[google-compute-default-zone\])\")

gcloud compute instances delete \--zone=\$ZONE \\

\--delete-disks=all default-us-vm

\######

These commands generate more activity to view in the audit logs exported
to BigQuery.

2.  Press **ENTER**.

When prompted, enter **Y**, and press **ENTER**. Notice you created two
buckets and deleted a Compute Engine instance.

3.  When the prompt appears after a few minutes, continue by entering
    the following commands into the Cloud Shell terminal:

gcloud storage rm \--recursive gs://\$DEVSHELL_PROJECT_ID

gcloud storage rm \--recursive gs://\$DEVSHELL_PROJECT_ID-test

Copied!

content_copy

4.  Press **ENTER**.

Notice you deleted both buckets.

![image](https://github.com/user-attachments/assets/b96312f6-a1e3-4e4e-ae23-10d3ed7319e0)


## Task 4. Sign in as the second user

I\'ll need to switch Google Cloud accounts by logging into the Google
Cloud console using the second user account provided in the **Lab
Details** panel. I will use this user account to analyze the logs.

1.  In the Google Cloud console, click on the user icon in the top-right
    corner of the screen, and then click **Add account**.

2.  Navigate back to the **Lab Details** panel, copy the **Google Cloud
    username
    2:** **student-00-fda86d86c105@qwiklabs.net** and **password**.
    Then, paste the username and password into the Google Cloud
    console **Sign in** dialog.

## Task 5. Analyze the Admin Activity logs

In this task, I\'ll review the Admin activity logs generated in the
previous task. My goal is to identify and apply filters to isolate logs
that may indicate suspicious activity. This will enable me to export
this subset of logs and streamline the process of analyzing them for
potential issues.

Admin Activity logs record the log entries for API calls or other
administrative actions that modify the configuration or metadata of
resources. For example, the logs record when VM instances and App Engine
applications are created when permissions are changed.

1.  In the Google Cloud console, click the **Navigation
    menu** ![image](https://github.com/user-attachments/assets/5680d524-56c2-4b1b-9430-463107b473ab)


2.  Select **Logging \> Logs Explorer**. The **Logs Explorer** page
    opens. (You may need to expand the **More Products** drop-down menu
    within the **Navigation** menu and locate Logging
    under **Operations**.)

3.  Ensure that the **Show query** toggle button is activated. This
    opens the **Query builder** field.

4.  Copy and paste the following command into the **Query
    builder** field. Notice your Google Cloud project ID, project ID in
    the command.

\####

logName =
(\"projects/qwiklabs-gcp-01-c4796fed48e3/logs/cloudaudit.googleapis.com%2Factivity\")

\####

5.  Click **Run query**.

![image](https://github.com/user-attachments/assets/e47f3a58-df38-47bd-9c99-82a9c1b66420)


6.  In the **Query results**, locate the log entry indicating that a
    Cloud Storage bucket was deleted, it will contain
    the **storage.buckets.delete** summary field. Summary fields are
    included in the log results to highlight important information about
    the log entry.

This entry refers to **storage.googleapis.com**, which calls
the **storage.buckets.delete** method to delete a bucket. The bucket
name is the same name as your project id: qwiklabs-gcp-01-c4796fed48e3.

![image](https://github.com/user-attachments/assets/efbce0d3-fc2e-488e-a0e7-bedf4a3b5d7b)


7.  Within this entry, click on the **storage.googleapis.com** text, and
    select **Show matching entries**. The **Query results** should now
    display only six entries related to created and deleted cloud
    storage buckets.

![image](https://github.com/user-attachments/assets/74f596b3-b850-4f4a-9596-7e654a0baa1e)


8.  In the Query editor field, notice
    the **protoPayload.serviceName=\"storage.googleapis.com\"** line was
    added to the query builder, this filters your query to entries only
    matching **storage.googleapis.com**.

9.  Within those query results, click **storage.buckets.delete** in one
    of the entries, and select **Show matching entries.**

10. 

![image](https://github.com/user-attachments/assets/a15e7bbb-1416-427f-bfe8-b594427feb56)

Notice another line was added to the **Query builder** text:

logName =
(\"projects/qwiklabs-gcp-01-c4796fed48e3/logs/cloudaudit.googleapis.com%2Factivity\")

protoPayload.serviceName=\"storage.googleapis.com\"

protoPayload.methodName=\"storage.buckets.delete\"

The **Query results** should now display all entries related to deleted
Cloud Storage buckets. You can use this technique to easily locate
specific events.

10. In the **Query results**, expand a **storage.buckets.delete** event
    by clicking the expand arrow **\>** next to the line:

![image](https://github.com/user-attachments/assets/dad671dc-3c2c-4139-a1ab-89bbbfaa8044)


11. Expand the **authenticatitonInfo** field by clicking the expand
    arrow **\>** next to the line:

![image](https://github.com/user-attachments/assets/fc9e393d-5ac9-47ca-a3e1-3eb4e4430703)


Notice the **principalEmail** field which displays the email address of
the user account that performed this action which is the user 1 account
you used to generate the user activity.

![image](https://github.com/user-attachments/assets/415af6f9-0878-4331-a2ac-bbdac2718314)


## Task 6. Use BigQuery to analyze the audit logs

I\'ve generated and exported logs to a BigQuery dataset. In this task,
you\'ll analyze the logs using the Query editor.

1.  In the Google Cloud console, click the **Navigation
    menu** ![image](https://github.com/user-attachments/assets/f33c883f-8a04-4c10-975a-565f395af8e4)


2.  Click **BigQuery**.

<!-- -->

3.  In the **Explorer** pane, click the expand arrow beside your
    project, qwiklabs-gcp-01-c4796fed48e3.
    The **auditlogs_dataset** dataset is displayed.

**Next**, verify that the BigQuery dataset has appropriate permissions
to allow the export writer to store log entries.

4.  Click the **auditlogs_dataset** dataset.

![image](https://github.com/user-attachments/assets/6ad06118-f5d1-4af5-9ec2-0cd61e2e69dc)


5.  In the auditlogs_dataset toolbar, click the **Sharing** dropdown
    menu, and select **Permissions**.

6.  On the **Share permission for \"auditlogs_dataset\"** page, expand
    the **BigQuery Data Editor** section.

![image](https://github.com/user-attachments/assets/f937b930-6920-40cb-8326-ab8e50880a85)


8.  Confirm that the service account used for log exports is a listed
    permission. The service account is similar to:
    service-xxxxxxxx@gcp-sa-logging.iam.gserviceaccount.com

> This permission is assigned automatically when log exports are
> configured so this is a useful way to check that log exports have been
> configured.

9.  Click **Close** to close the **Share Dataset** window.

10. In the **Explorer** pane, click the expander arrow next to
    the **auditlogs_dataset** dataset to view
    the **cloudaudit_googleapis_com_acitivty** table. This table
    contains your exported logs.

11. Select the **cloudaudit_googleapis_com_acitivty** table. The table
    schema displays. Take a moment to review the table schema and
    details.

12. Expand the **Query** drop-down menu and select **In new tab**.
![image](https://github.com/user-attachments/assets/56585ff4-57bb-4077-bab0-790221c72cad)


12. In the Untitled tab of the query builder, delete any existing text
    and copy and paste the following command:

\#####

SELECT

timestamp,

resource.labels.instance_id,

protopayload_auditlog.authenticationInfo.principalEmail,

protopayload_auditlog.resourceName,

protopayload_auditlog.methodName

FROM

\`auditlogs_dataset.cloudaudit_googleapis_com_activity\_\*\`

WHERE

PARSE_DATE(\'%Y%m%d\', \_TABLE_SUFFIX) BETWEEN

DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) AND

CURRENT_DATE()

AND resource.type = \"gce_instance\"

AND operation.first IS TRUE

AND protopayload_auditlog.methodName = \"v1.compute.instances.delete\"

ORDER BY

timestamp,

resource.labels.instance_id

LIMIT

1000;

\####

This query returns the users that deleted virtual machines in the last 7
days.

13. Click **Run**.

After a couple of seconds, BigQuery will return each time a user deleted
a Compute Engine virtual machine within the past 7 days. You should
notice one entry, which is the activity you generated in the previous
tasks as user 1. Remember, BigQuery shows only the activity that
occurred after you created the export.

![image](https://github.com/user-attachments/assets/b001cbd1-133a-4cdc-b709-c1a82d266f12)

14. Replace the previous query in the **Untitled** tab with the
    following:

\####

SELECT

timestamp,

resource.labels.bucket_name,

protopayload_auditlog.authenticationInfo.principalEmail,

protopayload_auditlog.resourceName,

protopayload_auditlog.methodName

FROM

\`auditlogs_dataset.cloudaudit_googleapis_com_activity\_\*\`

WHERE

PARSE_DATE(\'%Y%m%d\', \_TABLE_SUFFIX) BETWEEN

DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) AND

CURRENT_DATE()

AND resource.type = \"gcs_bucket\"

AND protopayload_auditlog.methodName = \"storage.buckets.delete\"

ORDER BY

timestamp,

resource.labels.instance_id

LIMIT

1000;

\######

This query returns the users that deleted Cloud Storage buckets in the
last 7 days. You should notice two entries, which is the activity you
generated in the previous tasks as user 1.

15. Click **Run**.

![image](https://github.com/user-attachments/assets/99cb27dd-84d4-4a25-b110-9825b20a04fd)

The ability to analyze audit logs in BigQuery is very powerful. In this
activity, you viewed just two examples of querying audit logs.

## Conclusion

This lab strengthened my skills in **Google Cloud audit log
investigation, incident recreation for analysis, and using BigQuery for
large-scale log analysis**. It provided valuable experience in the
initial phases of incident response, focusing on understanding malicious
activity through log artifacts. The outcome was a **clearer
understanding of unauthorized access patterns** and the practical
ability to use cloud logging and BigQuery for effective security
incident investigation.
